{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30673,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook34142e9ab8",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prashant501Tyagi/Hackathon_file/blob/main/notebook34142e9ab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM Project to Build and Fine Tune a Large Language Model**\n",
        "\n",
        "In today's data-driven world, the ability to process and generate natural language text at scale has become a transformative force across industries. Large Language Models (LLMs) represent a cutting-edge advancement in natural language processing, enabling businesses to extract valuable insights, automate tasks, and enhance user experiences. By harnessing the power of LLMs, organizations can improve customer service, automate content creation, and gain a competitive edge in the digital landscape.\n",
        "\n",
        "This project builds the foundation for Large Language Models by diving deep into the details of their inner workings. Moreover, It shows how to optimize their use through prompt engineering and fine-tuning techniques such as LoRA.\n",
        "\n",
        "Prompt engineering techniques involve crafting specific instructions or queries given to the language model to influence its output will be introduced to guide LLMs in generating desired responses through zero-shot, one-shot, and few-shot inferences.\n",
        "\n",
        "Fine-tuning entails training a pre-trained language model on a specific task or dataset to adapt it for a particular application. It explores full fine-tuning and Parameter Efficient Fine Tuning (PEFT), a technique that optimizes the fine-tuning process by focusing on a subset of the model's parameters, making it more resource-efficient.\n",
        "\n",
        "The project also involves the application of Retrieval Augmented Generation (RAG) using OpenAI's GPT-3.5 Turbo, resulting in the development of a chatbot for online shopping for knowledge grounding. Knowledge grounding with Retrieval Augmented Generation (RAG) is implemented to mitigate hallucinations and provide trustworthy and reliable responses. This is achieved by incorporating information from external sources to validate and support the generated text.\n",
        "\n",
        "For example, in the context of an e-commerce chatbot using RAG, knowledge grounding ensures that product information, availability, and prices are sourced from a trusted database or e-commerce platform. This prevents the chatbot from generating inaccurate or fictional details and instead provides responses based on real-world data.\n",
        "\n",
        "\n",
        "![image](https://images.pexels.com/photos/18069697/pexels-photo-18069697/free-photo-of-an-artist-s-illustration-of-artificial-intelligence-ai-this-illustration-depicts-language-models-which-generate-text-it-was-created-by-wes-cockx-as-part-of-the-visualising-ai-project-l.png?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1)"
      ],
      "metadata": {
        "id": "hq29eB8Edsbb"
      },
      "id": "hq29eB8Edsbb"
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:46:42.099779Z",
          "iopub.execute_input": "2024-04-09T05:46:42.100236Z",
          "iopub.status.idle": "2024-04-09T05:46:42.106553Z",
          "shell.execute_reply.started": "2024-04-09T05:46:42.100202Z",
          "shell.execute_reply": "2024-04-09T05:46:42.105055Z"
        },
        "trusted": true,
        "id": "sx6Igfzsdsbe"
      },
      "execution_count": null,
      "outputs": [],
      "id": "sx6Igfzsdsbe"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:46:42.108729Z",
          "iopub.execute_input": "2024-04-09T05:46:42.109078Z",
          "iopub.status.idle": "2024-04-09T05:46:58.42725Z",
          "shell.execute_reply.started": "2024-04-09T05:46:42.109048Z",
          "shell.execute_reply": "2024-04-09T05:46:58.426023Z"
        },
        "trusted": true,
        "id": "cNxXd5G0dsbf",
        "outputId": "18b68027-1e73-4f77-893c-551c9664b8d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-04-09 05:46:44.481166: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-09 05:46:44.481355: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-09 05:46:44.676339: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "2.15.0\n",
          "output_type": "stream"
        }
      ],
      "id": "cNxXd5G0dsbf"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:53.311111Z",
          "iopub.execute_input": "2024-04-09T05:47:53.31159Z",
          "iopub.status.idle": "2024-04-09T05:48:09.011566Z",
          "shell.execute_reply.started": "2024-04-09T05:47:53.311549Z",
          "shell.execute_reply": "2024-04-09T05:48:09.009779Z"
        },
        "trusted": true,
        "id": "4dX0_YExdsbg",
        "outputId": "3ef59931-0651-4318-c3f2-7116b5a35bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.21.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\n",
          "output_type": "stream"
        }
      ],
      "id": "4dX0_YExdsbg"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "s02VI1KKd-7c",
        "outputId": "3b58cd24-0b6d-44a2-9393-547ef048b33e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "s02VI1KKd-7c",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.3)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, responses, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 evaluate-0.4.1 multiprocess-0.70.16 responses-0.18.0 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import evaluate\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import (AutoModelForSeq2SeqLM, AutoModelForCausalLM,\n",
        "                          AutoTokenizer, GenerationConfig, TrainingArguments, Trainer)\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:48:09.014704Z",
          "iopub.execute_input": "2024-04-09T05:48:09.01525Z",
          "iopub.status.idle": "2024-04-09T05:48:16.068483Z",
          "shell.execute_reply.started": "2024-04-09T05:48:09.015196Z",
          "shell.execute_reply": "2024-04-09T05:48:16.066938Z"
        },
        "trusted": true,
        "id": "wG5KL8ZPdsbg"
      },
      "execution_count": 3,
      "outputs": [],
      "id": "wG5KL8ZPdsbg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Refresher**"
      ],
      "metadata": {
        "id": "3Gb1U0VDdsbg"
      },
      "id": "3Gb1U0VDdsbg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Recurrent Neural Networks**\n",
        "\n",
        "RNN were created because there were a few issues in the feed-forward neural network:\n",
        "\n",
        "Cannot handle sequential data\n",
        "Considers only the current input\n",
        "Cannot memorize previous inputs\n",
        "The solution to these issues is the RNN. An RNN can handle sequential data, accepting the current input data, and previously received inputs. RNNs can memorize previous inputs due to their internal memory.\n",
        "\n",
        "RNN works on the principle of saving the output of a particular layer and feeding this back to the input in order to predict the output of the layer.\n",
        "\n",
        "Below is how you can convert a Feed-Forward Neural Network into a Recurrent Neural Network\n",
        "\n",
        "<img src=\"images/rnn.png\"\n",
        "     align=\"center\"\n",
        "     width=\"700\" />\n",
        "\n",
        "\n",
        "The nodes in different layers of the neural network are compressed to form a single layer of recurrent neural networks. A, B, and C are the parameters of the network.\n",
        "\n",
        "<img src=\"images/rnn_animation.gif\"\n",
        "     align=\"center\"\n",
        "     width=\"450\" />\n",
        "\n",
        "\n",
        "The four commonly used types of Recurrent Neural Networks are:\n",
        "\n",
        "**One-to-One**: The simplest type of RNN is One-to-One, which allows a single input and a single output. It has fixed input and output sizes and acts as a traditional neural network. The One-to-One application can be found in Image Classification.\n",
        "\n",
        "**One-to-Many**: One-to-Many is a type of RNN that gives multiple outputs when given a single input. It takes a fixed input size and gives a sequence of data outputs. Its applications can be found in Music Generation and Image Captioning.\n",
        "\n",
        "**Many-to-One**: Many-to-One is used when a single output is required from multiple input units or a sequence of them. It takes a sequence of inputs to display a fixed output. Sentiment Analysis is a common example of this type of Recurrent Neural Network.\n",
        "\n",
        "**Many-to-Many**: Many-to-Many is used to generate a sequence of output data from a sequence of input units.\n",
        "\n",
        "This type of RNN is further divided into ﻿the following two subcategories:\n",
        "\n",
        "    1. Equal Unit Size: In this case, the number of both the input and output units is the same. A common application can be found in Name-Entity Recognition.\n",
        "    2. Unequal Unit Size: In this case, inputs and outputs have different numbers of units. Its application can be found in Machine Translation.\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"images/types_rnn.png\"\n",
        "     align=\"center\"\n",
        "     width=\"750\" />"
      ],
      "metadata": {
        "id": "zunjG2B9dsbh"
      },
      "id": "zunjG2B9dsbh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LSTM**\n",
        "\n",
        "Now, even though RNNs are quite powerful, they suffer from Vanishing gradient problem which hinders them from using long term information, like they are good for storing memory 3-4 instances of past iterations but larger number of instances don't provide good results so we don't just use regular RNNs. Instead, we use a better variation of RNNs: Long Short Term Networks(LSTM).\n",
        "\n",
        "**What is Vanishing Gradient problem?**\n",
        "\n",
        "Vanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. As one example of the problem cause, traditional activation functions such as the hyperbolic tangent function have gradients in the range (0, 1), and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the \"front\" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n while the front layers train very slowly.\n",
        "\n",
        "<img src=\"images/decay.png\"\n",
        "     align=\"center\"\n",
        "     width=\"750\" />\n",
        "\n",
        "\n",
        "**Fixing the Vanishing/Exploding Gradient with LSTMs**\n",
        "\n",
        "Long short-term memory (LSTM) units (or blocks) are a building unit for layers of a recurrent neural network (RNN). A RNN composed of LSTM units is often called an LSTM network. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell is responsible for \"remembering\" values over arbitrary time intervals; hence the word \"memory\" in LSTM. Each of the three gates can be thought of as a \"conventional\" artificial neuron, as in a multi-layer (or feedforward) neural network: that is, they compute an activation (using an activation function) of a weighted sum. Intuitively, they can be thought as regulators of the flow of values that goes through the connections of the LSTM; hence the denotation \"gate\". There are connections between these gates and the cell.\n",
        "\n",
        "The expression long short-term refers to the fact that LSTM is a model for the short-term memory which can last for a long period of time. An LSTM is well-suited to classify, process and predict time series given time lags of unknown size and duration between important events. LSTMs were developed to deal with the exploding and vanishing gradient problem when training traditional RNNs.\n",
        "\n",
        "\n",
        "<img src=\"images/lstm.png\"\n",
        "     align=\"center\"\n",
        "     width=\"750\" />\n",
        "\n",
        "More on LSTMs: https://medium.com/deep-math-machine-learning-ai/chapter-10-1-deepnlp-lstm-long-short-term-memory-networks-with-math-21477f8e4235\n",
        "\n"
      ],
      "metadata": {
        "id": "J1RNUz1fdsbh"
      },
      "id": "J1RNUz1fdsbh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Attention Mechanism**\n",
        "\n",
        "In a typical neural network, all parts of the input sequence are treated equally. However, in many tasks, certain parts of the input may be more relevant than others. For example, in a sentence, the word \"dog\" might be more important than \"the\" for understanding the meaning.\n",
        "\n",
        "The attention mechanism addresses this by allowing the model to focus on specific parts of the input when processing it. It does this by assigning weights or scores to different elements of the input sequence. These weights reflect how much attention the model should pay to each element. The weighted sum of the input elements then becomes the basis for making predictions.\n",
        "\n",
        "To know more refer to the project [Build a Text Classification Model with Attention Mechanism NLP](https://www.projectpro.io/project-use-case/multi-class-text-classification-model-with-attention-mechanism-in-nlp)"
      ],
      "metadata": {
        "id": "heBZiHpHdsbh"
      },
      "id": "heBZiHpHdsbh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transformers**\n",
        "\n",
        "Transformers are a type of neural network architecture that heavily rely on attention mechanisms. They were introduced in the paper [\"Attention is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf).\n",
        "\n",
        "Quoting from the paper:\n",
        "\n",
        "*The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.*\n",
        "\n",
        "Transformers are a class of powerful neural network architectures that have revolutionized natural language processing (NLP) and various other sequential data tasks. What sets Transformers apart is their unique attention mechanism, which allows them to process input data in parallel rather than sequentially. This means they can consider the relationships between all elements in a sequence simultaneously, making them exceptionally efficient. The attention mechanism works by assigning weights to different parts of the input, highlighting the most relevant information. Transformers are composed of multiple layers, each containing a multi-head self-attention sub-layer and a feedforward neural network sub-layer. These layers are stacked to form the core of the model.\n",
        "\n",
        "Refer to the project [NLP Project for Multi Class Text Classification using BERT Model](https://www.projectpro.io/project-use-case/nlp-project-for-multi-class-text-classification-using-bert) to implement transformers from scratch.\n",
        "\n",
        "Read more https://towardsdatascience.com/transformers-89034557de14"
      ],
      "metadata": {
        "id": "MKFc9XCGdsbi"
      },
      "id": "MKFc9XCGdsbi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenizers**\n",
        "\n",
        "Tokenization is a fundamental step in Natural Language Processing (NLP) that involves breaking down a text into smaller units, known as tokens. Tokens can be words, subwords, or characters, depending on the level of granularity required for the task at hand. In this tutorial, we'll explore various aspects of tokenization, including different tokenization techniques, libraries, and considerations for tokenizing text data."
      ],
      "metadata": {
        "id": "0Gt-R3OMdsbi"
      },
      "id": "0Gt-R3OMdsbi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Embeddings**\n",
        "\n",
        "Word embeddings are a crucial concept in Natural Language Processing (NLP). They represent words as numerical vectors in a continuous vector space. This tutorial will provide a conceptual understanding of word embeddings, their significance, and different methods for generating them.\n",
        "\n",
        "Word embeddings represent words in a continuous vector space, where similar words are positioned closer to each other. This enables algorithms to understand the context and relationships between words.\n",
        "\n",
        "Unlike one-hot encoding (a sparse representation), where each word is represented as a binary vector, word embeddings are dense vectors with continuous values. This allows for more nuanced representation of word meanings.\n",
        "\n",
        "Word embeddings capture semantic relationships between words. Words with similar meanings will have similar vector representations.\n",
        "\n",
        "\n",
        "To know more refer to the projects [Word2Vec and FastText Word Embedding with Gensim in Python](https://www.projectpro.io/project-use-case/word-embedding-with-word2vec-and-fasttext) and [Skip Gram Model Python Implementation for Word Embeddings](https://www.projectpro.io/project-use-case/skip-gram-model-word-embeddings-python)."
      ],
      "metadata": {
        "id": "1UsASMN_dsbi"
      },
      "id": "1UsASMN_dsbi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Generation**"
      ],
      "metadata": {
        "id": "mDuss4Kydsbi"
      },
      "id": "mDuss4Kydsbi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In recent years, there has been an increasing interest in open-ended language generation thanks to the rise of large transformer-based language models trained on millions of webpages, including OpenAI's ChatGPT and Meta's LLaMA. The results on conditioned open-ended language generation are impressive, having shown to generalize to new tasks, handle code, or take non-text data as input. Besides the improved transformer architecture and massive unsupervised training data, better decoding methods have also played an important role."
      ],
      "metadata": {
        "id": "43KXmKBbdsbi"
      },
      "id": "43KXmKBbdsbi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Currently most prominent decoding methods, mainly Greedy search, Beam search, and Sampling."
      ],
      "metadata": {
        "tags": [],
        "id": "FN3m66Uidsbi"
      },
      "id": "FN3m66Uidsbi"
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cpu'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:48:30.040192Z",
          "iopub.execute_input": "2024-04-09T05:48:30.041575Z",
          "iopub.status.idle": "2024-04-09T05:48:30.047418Z",
          "shell.execute_reply.started": "2024-04-09T05:48:30.041521Z",
          "shell.execute_reply": "2024-04-09T05:48:30.046008Z"
        },
        "trusted": true,
        "id": "Wm7oL1WAdsbi"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Wm7oL1WAdsbi"
    },
    {
      "cell_type": "code",
      "source": [
        "torch_device = torch.device(DEVICE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:48:31.055233Z",
          "iopub.execute_input": "2024-04-09T05:48:31.056219Z",
          "iopub.status.idle": "2024-04-09T05:48:31.061291Z",
          "shell.execute_reply.started": "2024-04-09T05:48:31.056172Z",
          "shell.execute_reply": "2024-04-09T05:48:31.059875Z"
        },
        "trusted": true,
        "id": "I5EK4XZ8dsbi"
      },
      "execution_count": null,
      "outputs": [],
      "id": "I5EK4XZ8dsbi"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# add the EOS token as PAD token to avoid warnings\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(torch_device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:48:32.008868Z",
          "iopub.execute_input": "2024-04-09T05:48:32.009287Z",
          "iopub.status.idle": "2024-04-09T05:48:37.670382Z",
          "shell.execute_reply.started": "2024-04-09T05:48:32.009255Z",
          "shell.execute_reply": "2024-04-09T05:48:37.669066Z"
        },
        "trusted": true,
        "id": "JVqYQUrqdsbj",
        "outputId": "dcf849c8-32b6-4944-974a-c9028c381b40",
        "colab": {
          "referenced_widgets": [
            "d725fe949fe44e5e98cafba06f169017",
            "165200d49c4e48eeb3772253de0c26ee",
            "832b2fc8687e443b943fe131bf573ddd",
            "886147fadd224ba094832c5de6e8412d",
            "8d2067a1bab54ad3ac6c3bc4e79642aa",
            "e6c69823525d4e32a1dfa1d33fefeae5",
            "86808f68203b4ddc943365150a981be4"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d725fe949fe44e5e98cafba06f169017"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "165200d49c4e48eeb3772253de0c26ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "832b2fc8687e443b943fe131bf573ddd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "886147fadd224ba094832c5de6e8412d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d2067a1bab54ad3ac6c3bc4e79642aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6c69823525d4e32a1dfa1d33fefeae5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86808f68203b4ddc943365150a981be4"
            }
          },
          "metadata": {}
        }
      ],
      "id": "JVqYQUrqdsbj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Greedy Search"
      ],
      "metadata": {
        "id": "OGnwU0TKdsbj"
      },
      "id": "OGnwU0TKdsbj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Greedy search is the simplest decoding method. It selects the word with the highest probability as its next word:"
      ],
      "metadata": {
        "id": "GOWcxBz1dsbj"
      },
      "id": "GOWcxBz1dsbj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![title](images/greedy_search.png)"
      ],
      "metadata": {
        "tags": [],
        "id": "hQA3oZ_bdsbj"
      },
      "id": "hQA3oZ_bdsbj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting from the word \"The\", the algorithm greedily chooses the next word of highest probability \"nice\" and so on, so that the final generated word sequence is (\"The\", \"nice\", \"woman\")\n",
        "having an overall probability of 0.5×0.4=0.2\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oz2AVvZfdsbj"
      },
      "id": "oz2AVvZfdsbj"
    },
    {
      "cell_type": "code",
      "source": [
        "# encode context the generation is conditioned on\n",
        "model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(torch_device)\n",
        "\n",
        "# generate 40 new tokens\n",
        "greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-04-09T05:48:47.66067Z",
          "iopub.execute_input": "2024-04-09T05:48:47.661087Z",
          "iopub.status.idle": "2024-04-09T05:48:49.405996Z",
          "shell.execute_reply.started": "2024-04-09T05:48:47.661054Z",
          "shell.execute_reply": "2024-04-09T05:48:49.404711Z"
        },
        "trusted": true,
        "id": "XlE370Gkdsbj",
        "outputId": "9de5fa7c-a268-4874-b642-808964f16065"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Output:\n----------------------------------------------------------------------------------------------------\nI enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n\nI'm not sure\n",
          "output_type": "stream"
        }
      ],
      "id": "XlE370Gkdsbj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have generated our first short text with GPT2!\n",
        "\n",
        "The generated words following the context are reasonable, but the model quickly start repeating itself! This is a very common problem in language generation in general and seems to be even more so in greedy and beam search. The major drawback of greedy search though is that it misses high probability words hidden behind low probability word as can be seen in the sketch above."
      ],
      "metadata": {
        "id": "eAlCOz6sdsbj"
      },
      "id": "eAlCOz6sdsbj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word \"has\" with its high conditional probability of 0.9 hidden behind the word \"dog\", which has only the second-highest conditional probability, so that greedy search misses the word sequence \"The\", \"dog\", \"has\"."
      ],
      "metadata": {
        "id": "Yd5usCy9dsbj"
      },
      "id": "Yd5usCy9dsbj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Beam Search**"
      ],
      "metadata": {
        "id": "vH8jhWsWdsbj"
      },
      "id": "vH8jhWsWdsbj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely num_beams of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. Let's illustrate with num_beams=2:"
      ],
      "metadata": {
        "id": "O-O_3wdXdsbj"
      },
      "id": "O-O_3wdXdsbj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![title](images/beam_search.png)"
      ],
      "metadata": {
        "tags": [],
        "id": "WcqOVU64dsbj"
      },
      "id": "WcqOVU64dsbj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "At time step 1, besides the most likely hypothesis (\"The\", \"nice\"), beam search also keeps track of the second most likely one (\"The\", \"dog\"). At time step 2, beam search finds that the word sequence (\"The\", \"dog\", \"has\") with 0.36 higher probability than (\"the\", \"nice\", \"woman\") which has 0.2. Great, it has found the most likely word sequence in the example."
      ],
      "metadata": {
        "id": "vB4adFm6dsbj"
      },
      "id": "vB4adFm6dsbj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beam search will always find an output sequence with higher probability than greedy search, but its not guaranteed to find the most likely output"
      ],
      "metadata": {
        "id": "IJXvtQ0Ddsbj"
      },
      "id": "IJXvtQ0Ddsbj"
    },
    {
      "cell_type": "code",
      "source": [
        "# activate beam search and early_stopping\n",
        "beam_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    num_beams=2,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-04-09T05:48:55.697299Z",
          "iopub.execute_input": "2024-04-09T05:48:55.697742Z",
          "iopub.status.idle": "2024-04-09T05:48:58.532622Z",
          "shell.execute_reply.started": "2024-04-09T05:48:55.697707Z",
          "shell.execute_reply": "2024-04-09T05:48:58.531323Z"
        },
        "trusted": true,
        "id": "p9Z7db1Jdsbk",
        "outputId": "b2f85c09-df17-45aa-a09f-9099b8c04e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Output:\n----------------------------------------------------------------------------------------------------\nI enjoy walking with my cute dog, but I don't think I'll ever be able to walk with my dog again.\n\nI'm not sure if I'll ever be able to walk with my dog again, but I don\n",
          "output_type": "stream"
        }
      ],
      "id": "p9Z7db1Jdsbk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the result is arguably more fluent, the output still includes repetitions of the same word sequences. One of the available remedies is to introduce n-grams. The most common n-grams penalty makes sure that no n-gram appears twice by manually setting the probability of next words that could create an already seen n-gram to 0."
      ],
      "metadata": {
        "id": "VpPPFhHndsbk"
      },
      "id": "VpPPFhHndsbk"
    },
    {
      "cell_type": "code",
      "source": [
        "# set no_repeat_ngram_size to 2\n",
        "beam_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-04-09T05:49:02.772782Z",
          "iopub.execute_input": "2024-04-09T05:49:02.773188Z",
          "iopub.status.idle": "2024-04-09T05:49:06.07941Z",
          "shell.execute_reply.started": "2024-04-09T05:49:02.773156Z",
          "shell.execute_reply": "2024-04-09T05:49:06.078Z"
        },
        "trusted": true,
        "id": "VX7ZyNTrdsbk",
        "outputId": "2f5cd560-0eda-4976-f0ee-0cb06ba87fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Output:\n----------------------------------------------------------------------------------------------------\nI enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n\nI've been thinking about this for a while now, and I think it's time for me to\n",
          "output_type": "stream"
        }
      ],
      "id": "VX7ZyNTrdsbk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the repetition does not appear anymore. Nevertheless, n-gram penalties have to be used with care. An article generated about the city New York should not use a 2-gram penalty or otherwise, the name of the city would only appear once in the whole text!"
      ],
      "metadata": {
        "id": "DQ8KtfFSdsbk"
      },
      "id": "DQ8KtfFSdsbk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another important feature about beam search is that we can compare the top beams after generation and choose the generated beam that fits our purpose best."
      ],
      "metadata": {
        "id": "mfxI08_Tdsbk"
      },
      "id": "mfxI08_Tdsbk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In transformers, we simply set the parameter num_return_sequences to the number of highest scoring beams that should be returned. Make sure though that num_return_sequences <= num_beams!"
      ],
      "metadata": {
        "id": "wq-vpr_Idsbk"
      },
      "id": "wq-vpr_Idsbk"
    },
    {
      "cell_type": "code",
      "source": [
        "# set return_num_sequences > 1\n",
        "beam_outputs = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_return_sequences=5,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# now we have 3 output sequences\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-04-09T05:49:07.475026Z",
          "iopub.execute_input": "2024-04-09T05:49:07.475521Z",
          "iopub.status.idle": "2024-04-09T05:49:10.794961Z",
          "shell.execute_reply.started": "2024-04-09T05:49:07.475479Z",
          "shell.execute_reply": "2024-04-09T05:49:10.793463Z"
        },
        "trusted": true,
        "id": "7n6-TbL9dsbk",
        "outputId": "e91c1daf-f6af-4158-b180-bed1a4f77298"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Output:\n----------------------------------------------------------------------------------------------------\n0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n\nI've been thinking about this for a while now, and I think it's time for me to\n1: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n\nI've been thinking about this for a while now, and I think it's time for me to\n2: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n\nI've been thinking about this for a while now, and I think it's a good idea to\n3: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n\nI've been thinking about this for a while now, and I think it's time to take a\n4: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n\nI've been thinking about this for a while now, and I think it's a good idea.\n",
          "output_type": "stream"
        }
      ],
      "id": "7n6-TbL9dsbk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen, the five beam hypotheses are only marginally different to each other - which should not be too surprising when using only 5 beams."
      ],
      "metadata": {
        "id": "EUAp13BBdsbk"
      },
      "id": "EUAp13BBdsbk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sampling**"
      ],
      "metadata": {
        "id": "q6WfaxnDdsbk"
      },
      "id": "q6WfaxnDdsbk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In its most basic form, sampling means randomly picking the next word according to its conditional probability distribution"
      ],
      "metadata": {
        "id": "HAvAKvzodsbo"
      },
      "id": "HAvAKvzodsbo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "![title](images/sampling_search.png)"
      ],
      "metadata": {
        "id": "8ZvWBBN5dsbo"
      },
      "id": "8ZvWBBN5dsbo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "It becomes obvious that language generation using sampling is not deterministic anymore. The word (\"car\") is sampled from the conditioned probability distribution P(w|\"The\"), followed by sampling (\"drives\") from P(w|\"The\", \"car\")"
      ],
      "metadata": {
        "id": "mh9_PWUfdsbo"
      },
      "id": "mh9_PWUfdsbo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In transformers, we set do_sample=True and deactivate Top-K sampling (more on this later) via top_k=0. In the following, we will fix the random seed for illustration purposes. Feel free to change the set_seed argument to obtain different results, or to remove it for non-determinism."
      ],
      "metadata": {
        "id": "tSuOaWIndsbo"
      },
      "id": "tSuOaWIndsbo"
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "from transformers import set_seed\n",
        "set_seed(42)\n",
        "\n",
        "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
        "sample_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-04-09T05:49:15.439803Z",
          "iopub.execute_input": "2024-04-09T05:49:15.440831Z",
          "iopub.status.idle": "2024-04-09T05:49:16.983347Z",
          "shell.execute_reply.started": "2024-04-09T05:49:15.440788Z",
          "shell.execute_reply": "2024-04-09T05:49:16.982495Z"
        },
        "trusted": true,
        "id": "3MRw-e3Ldsbo",
        "outputId": "5efaf751-01eb-471f-b4a8-4c61573df0bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Output:\n----------------------------------------------------------------------------------------------------\nI enjoy walking with my cute dog but what I love about being a dog cat person is being a pet being with people who can treat you. I feel happy to be such a pet person and get to meet so many people. I\n",
          "output_type": "stream"
        }
      ],
      "id": "3MRw-e3Ldsbo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting! The text seems alright - but when taking a closer look, it is not very coherent and doesn't sound like it was written by a human. That is the big problem when sampling word sequences: The models often generate incoherent gibberish"
      ],
      "metadata": {
        "id": "MlR4gWnydsbo"
      },
      "id": "MlR4gWnydsbo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A trick is to make the distribution sharper (increasing the likelihood of high probability words and decreasing the likelihood of low probability words) by lowering the so called temperature of the softmax"
      ],
      "metadata": {
        "id": "TEloADEgdsbo"
      },
      "id": "TEloADEgdsbo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"images/sampling_search_with_temp.png\" width=\"800\" height=\"400\">"
      ],
      "metadata": {
        "id": "Ae_ii56qdsbo"
      },
      "id": "Ae_ii56qdsbo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The conditional next word distribution of step T=1 becomes much sharper leaving almost no chance for word (\"car\") to be selected."
      ],
      "metadata": {
        "id": "Q89eWdm1dsbp"
      },
      "id": "Q89eWdm1dsbp"
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "set_seed(42)\n",
        "\n",
        "# use temperature to decrease the sensitivity to low probability candidates\n",
        "sample_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_k=0,\n",
        "    temperature=0.6,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-04-09T05:51:23.975985Z",
          "iopub.execute_input": "2024-04-09T05:51:23.976516Z",
          "iopub.status.idle": "2024-04-09T05:51:25.505514Z",
          "shell.execute_reply.started": "2024-04-09T05:51:23.976477Z",
          "shell.execute_reply": "2024-04-09T05:51:25.504183Z"
        },
        "trusted": true,
        "id": "HQNzAbJjdsbp",
        "outputId": "d54bb7fc-9870-446b-91e3-590cdcaaa589"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Output:\n----------------------------------------------------------------------------------------------------\nI enjoy walking with my cute dog but I also love the fact that my cat is not a dog. She is a good, loving dog. I do not like to be held back by other dogs but I think that I have to\n",
          "output_type": "stream"
        }
      ],
      "id": "HQNzAbJjdsbp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "There were less weird n-grams and the output is a bit more coherent now. However, while applying temperature can make a distribution less random, in its limit, when setting temperature -> 0, temperature scaled sampling becomes equal to greedy decoding and will suffer from the same problems as before."
      ],
      "metadata": {
        "id": "4IlwcWkWdsbp"
      },
      "id": "4IlwcWkWdsbp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Top-K Sampling**"
      ],
      "metadata": {
        "id": "-crqpry9dsbp"
      },
      "id": "-crqpry9dsbp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Top-K sampling, the K most likely next words are filtered and the probability mass is redistributed among only those K next words. GPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation."
      ],
      "metadata": {
        "id": "3d60S3bEdsbp"
      },
      "id": "3d60S3bEdsbp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"images/top_k_sampling.png\" width=\"1200\" height=\"600\">"
      ],
      "metadata": {
        "id": "5ynfXgFTdsbp"
      },
      "id": "5ynfXgFTdsbp"
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "set_seed(42)\n",
        "\n",
        "# set top_k to 50\n",
        "sample_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=35,\n",
        "    do_sample=True,\n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-04-09T05:51:42.405981Z",
          "iopub.execute_input": "2024-04-09T05:51:42.406411Z",
          "iopub.status.idle": "2024-04-09T05:51:43.763421Z",
          "shell.execute_reply.started": "2024-04-09T05:51:42.406378Z",
          "shell.execute_reply": "2024-04-09T05:51:43.7625Z"
        },
        "trusted": true,
        "id": "0ebsrZQCdsbp",
        "outputId": "648e6ac1-b867-4840-e27f-933d0bc6ae74"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Output:\n----------------------------------------------------------------------------------------------------\nI enjoy walking with my cute dog but what I love about being a dog is I see a beautiful pet being cared for – I love having the opportunity to see her every day so I feel very privileged to have\n",
          "output_type": "stream"
        }
      ],
      "id": "0ebsrZQCdsbp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad at all! The text is arguably the most human-sounding text so far. One concern though with Top-K sampling is that it does not dynamically adapt the number of words that are filtered from the next word probability distribution. This can be problematic as some words might be sampled from a very sharp distribution (distribution on the right in the graph above), whereas others from a much more flat distribution (distribution on the left in the graph above)."
      ],
      "metadata": {
        "id": "dhBvUTARdsbp"
      },
      "id": "dhBvUTARdsbp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Top-p (nucleus) sampling**"
      ],
      "metadata": {
        "id": "QSXraIYYdsbp"
      },
      "id": "QSXraIYYdsbp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of sampling only from the most likely K words, in Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. This way, the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution. Ok, that was very wordy, let's visualize."
      ],
      "metadata": {
        "id": "EbqusDAgdsbp"
      },
      "id": "EbqusDAgdsbp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"images/top_p_sampling.png\" width=\"1200\" height=\"600\">"
      ],
      "metadata": {
        "id": "1QS4IeKqdsbp"
      },
      "id": "1QS4IeKqdsbp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having set p = 0.92, Top-p sampling picks the minimum number of words to exceed together p = 92% of the probability mass. In the first example, this included the 9 most likely words, whereas it only has to pick the top 3 words in the second example to exceed 92%."
      ],
      "metadata": {
        "id": "V_fFeFHtdsbp"
      },
      "id": "V_fFeFHtdsbp"
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "set_seed(42)\n",
        "\n",
        "# set top_k to 50\n",
        "sample_output = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=35,\n",
        "    do_sample=True,\n",
        "    top_p=0.92,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-04-09T05:51:50.509418Z",
          "iopub.execute_input": "2024-04-09T05:51:50.51016Z",
          "iopub.status.idle": "2024-04-09T05:51:52.076846Z",
          "shell.execute_reply.started": "2024-04-09T05:51:50.510122Z",
          "shell.execute_reply": "2024-04-09T05:51:52.075509Z"
        },
        "trusted": true,
        "id": "fL_FQGOddsbp",
        "outputId": "f2a7ca07-ad9b-4676-cae7-39c4d4abf4fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Output:\n----------------------------------------------------------------------------------------------------\nI enjoy walking with my cute dog but what I love about being a dog cat person is being a pet being with people who can treat you. I feel happy to be such a pet person and get to meet\n",
          "output_type": "stream"
        }
      ],
      "id": "fL_FQGOddsbp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "While in theory, Top-p seems more elegant than Top-K, both methods work well in practice. Top-p can also be used in combination with Top-K, which can avoid very low ranked words while allowing for some dynamic selection."
      ],
      "metadata": {
        "id": "g0gXQfGAdsbq"
      },
      "id": "g0gXQfGAdsbq"
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "set_seed(42)\n",
        "\n",
        "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
        "sample_outputs = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=40,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    num_return_sequences=3,\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "tags": [],
        "execution": {
          "iopub.status.busy": "2024-04-09T05:51:57.711181Z",
          "iopub.execute_input": "2024-04-09T05:51:57.711642Z",
          "iopub.status.idle": "2024-04-09T05:52:01.046045Z",
          "shell.execute_reply.started": "2024-04-09T05:51:57.711608Z",
          "shell.execute_reply": "2024-04-09T05:52:01.045029Z"
        },
        "trusted": true,
        "id": "_XZ4S8ondsbq",
        "outputId": "177370c8-5233-4bbc-d1a7-837025dd5d8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Output:\n----------------------------------------------------------------------------------------------------\n0: I enjoy walking with my cute dog but sometimes I get nervous when she is around. I've been told that with her alone, she will usually wander off and then try to chase me. It's nice to know that I have this\n1: I enjoy walking with my cute dog. I think she is the same one I like to walk with my dog, I think she is about as girly as my first dog. I hope we can find an apartment for her when we\n2: I enjoy walking with my cute dog, but there's so much to say about him that I am going to miss it all. He has been so supportive and even had my number in his bag.\n\nI hope I can say\n",
          "output_type": "stream"
        }
      ],
      "id": "_XZ4S8ondsbq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dialogue Summarization**"
      ],
      "metadata": {
        "id": "dO_ScVDsdsbq"
      },
      "id": "dO_ScVDsdsbq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this use case, we will be generating a summary of a dialogue with the pre-trained Large Language Model (LLM) FLAN-T5 from Hugging face."
      ],
      "metadata": {
        "id": "pGwr78Kpdsbq"
      },
      "id": "pGwr78Kpdsbq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's upload some simple dialogues from the DialogSum Hugging Face dataset. This dataset contains 10.000+ dialogues with the corresponding manually labeled summaries and topics."
      ],
      "metadata": {
        "id": "HEjWnlUudsbq"
      },
      "id": "HEjWnlUudsbq"
    },
    {
      "cell_type": "code",
      "source": [
        "torch_device = torch.device(DEVICE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:52:28.66564Z",
          "iopub.execute_input": "2024-04-09T05:52:28.66781Z",
          "iopub.status.idle": "2024-04-09T05:52:28.673618Z",
          "shell.execute_reply.started": "2024-04-09T05:52:28.667761Z",
          "shell.execute_reply": "2024-04-09T05:52:28.672206Z"
        },
        "trusted": true,
        "id": "JpxkyfaFdsbq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JpxkyfaFdsbq"
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface_dataset_name = 'knkarthick/dialogsum'\n",
        "dataset = load_dataset(huggingface_dataset_name)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:52:29.595566Z",
          "iopub.execute_input": "2024-04-09T05:52:29.596023Z",
          "iopub.status.idle": "2024-04-09T05:52:31.507502Z",
          "shell.execute_reply.started": "2024-04-09T05:52:29.595986Z",
          "shell.execute_reply": "2024-04-09T05:52:31.505264Z"
        },
        "trusted": true,
        "id": "-POynrlHdsbq",
        "outputId": "cab30750-f09e-427c-bb50-8eabc90968a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m huggingface_dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknkarthick/dialogsum\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhuggingface_dataset_name\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:1664\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1661\u001b[0m ignore_verifications \u001b[38;5;241m=\u001b[39m ignore_verifications \u001b[38;5;129;01mor\u001b[39;00m save_infos\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1664\u001b[0m builder_instance \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:1490\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m   1488\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m download_config\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;28;01melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1489\u001b[0m     download_config\u001b[38;5;241m.\u001b[39muse_auth_token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1490\u001b[0m dataset_module \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;66;03m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m builder_cls \u001b[38;5;241m=\u001b[39m import_main_class(dataset_module\u001b[38;5;241m.\u001b[39mmodule_path)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:1242\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[1;32m   1238\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1240\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1241\u001b[0m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1242\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1245\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a dataset script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or any data file in the same directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1246\u001b[0m     )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:1230\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m HubDatasetModuleFactoryWithScript(\n\u001b[1;32m   1216\u001b[0m                 path,\n\u001b[1;32m   1217\u001b[0m                 revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1220\u001b[0m                 dynamic_modules_path\u001b[38;5;241m=\u001b[39mdynamic_modules_path,\n\u001b[1;32m   1221\u001b[0m             )\u001b[38;5;241m.\u001b[39mget_module()\n\u001b[1;32m   1222\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1223\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHubDatasetModuleFactoryWithoutScript\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1230\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e1:  \u001b[38;5;66;03m# noqa: all the attempts failed, before raising the error we should check if the module is already cached.\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/load.py:846\u001b[0m, in \u001b[0;36mHubDatasetModuleFactoryWithoutScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    836\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muse_auth_token\n\u001b[1;32m    837\u001b[0m hfh_dataset_info \u001b[38;5;241m=\u001b[39m HfApi(config\u001b[38;5;241m.\u001b[39mHF_ENDPOINT)\u001b[38;5;241m.\u001b[39mdataset_info(\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    839\u001b[0m     revision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[1;32m    840\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    841\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100.0\u001b[39m,\n\u001b[1;32m    842\u001b[0m )\n\u001b[1;32m    843\u001b[0m patterns \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    844\u001b[0m     sanitize_patterns(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files)\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_files \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 846\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mget_patterns_in_dataset_repository\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhfh_dataset_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m )\n\u001b[1;32m    848\u001b[0m data_files \u001b[38;5;241m=\u001b[39m DataFilesDict\u001b[38;5;241m.\u001b[39mfrom_hf_repo(\n\u001b[1;32m    849\u001b[0m     patterns,\n\u001b[1;32m    850\u001b[0m     dataset_info\u001b[38;5;241m=\u001b[39mhfh_dataset_info,\n\u001b[1;32m    851\u001b[0m     allowed_extensions\u001b[38;5;241m=\u001b[39mALL_ALLOWED_EXTENSIONS,\n\u001b[1;32m    852\u001b[0m )\n\u001b[1;32m    853\u001b[0m infered_module_names \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    854\u001b[0m     key: infer_module_for_data_files(data_files_list, use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload_config\u001b[38;5;241m.\u001b[39muse_auth_token)\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, data_files_list \u001b[38;5;129;01min\u001b[39;00m data_files\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    856\u001b[0m }\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/data_files.py:471\u001b[0m, in \u001b[0;36mget_patterns_in_dataset_repository\u001b[0;34m(dataset_info)\u001b[0m\n\u001b[1;32m    469\u001b[0m resolver \u001b[38;5;241m=\u001b[39m partial(_resolve_single_pattern_in_dataset_repository, dataset_info)\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_data_files_patterns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset repository at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_info\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt contain any data file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    475\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/data_files.py:99\u001b[0m, in \u001b[0;36m_get_data_files_patterns\u001b[0;34m(pattern_resolver)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m patterns:\n\u001b[0;32m---> 99\u001b[0m         data_files \u001b[38;5;241m=\u001b[39m \u001b[43mpattern_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_files) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    101\u001b[0m             non_empty_splits\u001b[38;5;241m.\u001b[39mappend(split)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/data_files.py:303\u001b[0m, in \u001b[0;36m_resolve_single_pattern_in_dataset_repository\u001b[0;34m(dataset_info, pattern, allowed_extensions)\u001b[0m\n\u001b[1;32m    301\u001b[0m data_files_ignore \u001b[38;5;241m=\u001b[39m FILES_TO_IGNORE\n\u001b[1;32m    302\u001b[0m fs \u001b[38;5;241m=\u001b[39m HfFileSystem(repo_info\u001b[38;5;241m=\u001b[39mdataset_info)\n\u001b[0;32m--> 303\u001b[0m glob_iter \u001b[38;5;241m=\u001b[39m [PurePath(filepath) \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPurePath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(filepath)]\n\u001b[1;32m    304\u001b[0m matched_paths \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    305\u001b[0m     filepath\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filepath \u001b[38;5;129;01min\u001b[39;00m glob_iter\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filepath\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data_files_ignore \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filepath\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    308\u001b[0m ]\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allowed_extensions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/spec.py:606\u001b[0m, in \u001b[0;36mAbstractFileSystem.glob\u001b[0;34m(self, path, maxdepth, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m         depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    604\u001b[0m allpaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind(root, maxdepth\u001b[38;5;241m=\u001b[39mdepth, withdirs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 606\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[43mglob_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mends_with_sep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(pattern)\n\u001b[1;32m    609\u001b[0m out \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    610\u001b[0m     p: info\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p, info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(allpaths\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    619\u001b[0m }\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/utils.py:734\u001b[0m, in \u001b[0;36mglob_translate\u001b[0;34m(pat)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m part:\n\u001b[0;32m--> 734\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    735\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid pattern: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m can only be an entire path component\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m     )\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m part:\n\u001b[1;32m    738\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(_translate(part, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_sep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m, not_sep))\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid pattern: '**' can only be an entire path component"
          ],
          "ename": "ValueError",
          "evalue": "Invalid pattern: '**' can only be an entire path component",
          "output_type": "error"
        }
      ],
      "id": "-POynrlHdsbq"
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices = [40, 200]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:52:38.233543Z",
          "iopub.execute_input": "2024-04-09T05:52:38.233982Z",
          "iopub.status.idle": "2024-04-09T05:52:38.23946Z",
          "shell.execute_reply.started": "2024-04-09T05:52:38.233944Z",
          "shell.execute_reply": "2024-04-09T05:52:38.238404Z"
        },
        "trusted": true,
        "id": "a8Cieaj5dsbq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "a8Cieaj5dsbq"
    },
    {
      "cell_type": "code",
      "source": [
        "dash_line = \"-\".join('' for x in range(100))\n",
        "\n",
        "for i, index in enumerate(example_indices):\n",
        "    print(dash_line)\n",
        "    print('Example ', i+1)\n",
        "    print(dash_line)\n",
        "    print('Input Dialogue: ')\n",
        "    print(dataset['test'][index]['dialogue'])\n",
        "    print(dash_line)\n",
        "    print('Baseline Human Summary: ')\n",
        "    print(dataset['test'][index]['summary'])\n",
        "    print(dash_line)\n",
        "    print()\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.741593Z",
          "iopub.status.idle": "2024-04-09T05:47:02.742138Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.741858Z",
          "shell.execute_reply": "2024-04-09T05:47:02.741881Z"
        },
        "trusted": true,
        "id": "clDTidRBdsbq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "clDTidRBdsbq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FLAN-T5 Model**"
      ],
      "metadata": {
        "id": "WaJ4l7IRdsbq"
      },
      "id": "WaJ4l7IRdsbq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"images/flan2_architecture.jpg\" width=\"1000\" height=\"500\">"
      ],
      "metadata": {
        "id": "QKeoMT3Mdsbq"
      },
      "id": "QKeoMT3Mdsbq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models"
      ],
      "metadata": {
        "id": "K57wrkZCdsbq"
      },
      "id": "K57wrkZCdsbq"
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'google/flan-t5-base'\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.743482Z",
          "iopub.status.idle": "2024-04-09T05:47:02.744025Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.743745Z",
          "shell.execute_reply": "2024-04-09T05:47:02.743769Z"
        },
        "trusted": true,
        "id": "7UJyEUVqdsbq"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7UJyEUVqdsbq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "op1XUsDedsbq"
      },
      "id": "op1XUsDedsbq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"images/flan_t5_tasks.png\" width=\"900\" height=\"450\">"
      ],
      "metadata": {
        "id": "UPdC-GX1dsbr"
      },
      "id": "UPdC-GX1dsbr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### These models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size."
      ],
      "metadata": {
        "id": "BHJYpBt1dsbr"
      },
      "id": "BHJYpBt1dsbr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "fWqGjbMBdsbr"
      },
      "id": "fWqGjbMBdsbr"
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"What time is it, Tom?\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.746147Z",
          "iopub.status.idle": "2024-04-09T05:47:02.746716Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.746419Z",
          "shell.execute_reply": "2024-04-09T05:47:02.746462Z"
        },
        "trusted": true,
        "id": "3aYz9hScdsbr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3aYz9hScdsbr"
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "sentence_decoded = tokenizer.decode(sentence_encoded[\"input_ids\"][0], skip_special_tokens=True)\n",
        "\n",
        "print(f'ENCODED SENTENCE:\\n {sentence_encoded[\"input_ids\"][0]}')\n",
        "print(f'DECODED SENTENCE: {sentence_decoded}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.748705Z",
          "iopub.status.idle": "2024-04-09T05:47:02.749298Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.749002Z",
          "shell.execute_reply": "2024-04-09T05:47:02.749026Z"
        },
        "trusted": true,
        "id": "n3i38uUXdsbr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "n3i38uUXdsbr"
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_dialogues(example_indices, dataset, prompt = \"%s\"):\n",
        "    for i, index in enumerate(example_indices):\n",
        "        dialogue = dataset['test'][index]['dialogue']\n",
        "        summary = dataset['test'][index]['summary']\n",
        "\n",
        "        input = prompt % (dialogue)\n",
        "\n",
        "        inputs = tokenizer(input, return_tensors='pt')\n",
        "        pred = model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0]\n",
        "        output = tokenizer.decode(pred, skip_special_tokens=True)\n",
        "\n",
        "        print(dash_line)\n",
        "        print(f'Example {i+1}')\n",
        "        print(dash_line)\n",
        "        print(f'Input Prompt: \\n {dialogue}')\n",
        "        print(dash_line)\n",
        "        print(f'Baseline Human Summary: \\n {summary}')\n",
        "        print(dash_line)\n",
        "        print(f'Model Generation: \\n{output}\\n')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.754122Z",
          "iopub.status.idle": "2024-04-09T05:47:02.754647Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.754361Z",
          "shell.execute_reply": "2024-04-09T05:47:02.754411Z"
        },
        "trusted": true,
        "id": "qsXFawPSdsbr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "qsXFawPSdsbr"
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_dialogues(example_indices, dataset)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.755911Z",
          "iopub.status.idle": "2024-04-09T05:47:02.756313Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.756126Z",
          "shell.execute_reply": "2024-04-09T05:47:02.756142Z"
        },
        "trusted": true,
        "id": "Lqz9Iic_dsbr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Lqz9Iic_dsbr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero Shot Inference with an Instruction Prompt"
      ],
      "metadata": {
        "id": "fRRXQbhxdsbr"
      },
      "id": "fRRXQbhxdsbr"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f'Summarize the following conversation. \\n%s\\nSummary:'\n",
        "print(prompt)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.75806Z",
          "iopub.status.idle": "2024-04-09T05:47:02.758487Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.758268Z",
          "shell.execute_reply": "2024-04-09T05:47:02.758283Z"
        },
        "trusted": true,
        "id": "KQQq5oMsdsbr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "KQQq5oMsdsbr"
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_dialogues(example_indices, dataset, prompt)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.759707Z",
          "iopub.status.idle": "2024-04-09T05:47:02.760099Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.759913Z",
          "shell.execute_reply": "2024-04-09T05:47:02.759928Z"
        },
        "trusted": true,
        "id": "nSiAqeBmdsbr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "nSiAqeBmdsbr"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f'Dialogue: \\n%s\\n\\nWhat Happened?'\n",
        "print(prompt)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.761237Z",
          "iopub.status.idle": "2024-04-09T05:47:02.761652Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.761422Z",
          "shell.execute_reply": "2024-04-09T05:47:02.761437Z"
        },
        "trusted": true,
        "id": "xneZOQGYdsbr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "xneZOQGYdsbr"
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_dialogues(example_indices, dataset, prompt)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.762809Z",
          "iopub.status.idle": "2024-04-09T05:47:02.763195Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.76301Z",
          "shell.execute_reply": "2024-04-09T05:47:02.763025Z"
        },
        "trusted": true,
        "id": "_ccPPnIydsbr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_ccPPnIydsbr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One Shot Inference"
      ],
      "metadata": {
        "id": "8UsHN3NSdsbr"
      },
      "id": "8UsHN3NSdsbr"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(example_indices_full, example_index_to_summarize):\n",
        "    prompt = ''\n",
        "    for index in example_indices_full:\n",
        "        dialogue = dataset['test'][index]['dialogue']\n",
        "        summary = dataset['test'][index]['summary']\n",
        "\n",
        "        prompt += f\"\"\"Dialogue:\\n{dialogue}\\n\\nWhat was going on?\\n{summary}\\n\\n\\n\"\"\"\n",
        "        dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
        "\n",
        "    prompt += f'Dialogue:\\n{dialogue}\\n\\nWhat was going on?'\n",
        "    return prompt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.764208Z",
          "iopub.status.idle": "2024-04-09T05:47:02.764594Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.764393Z",
          "shell.execute_reply": "2024-04-09T05:47:02.764407Z"
        },
        "trusted": true,
        "id": "vtKZn3LCdsbr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "vtKZn3LCdsbr"
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices_full = [40]\n",
        "example_index_to_summarize = 200\n",
        "\n",
        "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
        "\n",
        "print(one_shot_prompt)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.76536Z",
          "iopub.status.idle": "2024-04-09T05:47:02.765787Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.765587Z",
          "shell.execute_reply": "2024-04-09T05:47:02.765604Z"
        },
        "trusted": true,
        "id": "EyaXaxvpdsbr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "EyaXaxvpdsbr"
    },
    {
      "cell_type": "code",
      "source": [
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(dash_line)\n",
        "print(f'Baseline Human Summary: \\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'Model Generation - One Shot:\\n{output}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.767469Z",
          "iopub.status.idle": "2024-04-09T05:47:02.767851Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.767664Z",
          "shell.execute_reply": "2024-04-09T05:47:02.767679Z"
        },
        "trusted": true,
        "id": "eTTgSb5Udsbs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "eTTgSb5Udsbs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few Shot Inference"
      ],
      "metadata": {
        "id": "tqBvbs2Ldsbs"
      },
      "id": "tqBvbs2Ldsbs"
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices_full = [40, 80, 120]\n",
        "example_index_to_summarize = 200\n",
        "\n",
        "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
        "\n",
        "print(few_shot_prompt)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.769346Z",
          "iopub.status.idle": "2024-04-09T05:47:02.769811Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.769601Z",
          "shell.execute_reply": "2024-04-09T05:47:02.76962Z"
        },
        "trusted": true,
        "id": "teMLoNb-dsbs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "teMLoNb-dsbs"
    },
    {
      "cell_type": "code",
      "source": [
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(dash_line)\n",
        "print(f'Baseline Human Summary: \\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'Model Generation - Few Shot:\\n{output}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.771249Z",
          "iopub.status.idle": "2024-04-09T05:47:02.771642Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.771454Z",
          "shell.execute_reply": "2024-04-09T05:47:02.771469Z"
        },
        "trusted": true,
        "id": "pvi0FsEgdsbs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pvi0FsEgdsbs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Fine Tuning"
      ],
      "metadata": {
        "id": "c8IpfI-Bdsbs"
      },
      "id": "c8IpfI-Bdsbs"
    },
    {
      "cell_type": "code",
      "source": [
        "torch_device = torch.device(DEVICE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.772941Z",
          "iopub.status.idle": "2024-04-09T05:47:02.773316Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.773135Z",
          "shell.execute_reply": "2024-04-09T05:47:02.77315Z"
        },
        "trusted": true,
        "id": "UNFi4u_xdsbs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "UNFi4u_xdsbs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset and LLM"
      ],
      "metadata": {
        "id": "RVIDY73hdsbs"
      },
      "id": "RVIDY73hdsbs"
    },
    {
      "cell_type": "code",
      "source": [
        "hugging_face_dataset_name = \"knkarthick/dialogsum\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.775176Z",
          "iopub.status.idle": "2024-04-09T05:47:02.775571Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.77536Z",
          "shell.execute_reply": "2024-04-09T05:47:02.775376Z"
        },
        "trusted": true,
        "id": "BZqx8agRdsbs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "BZqx8agRdsbs"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(hugging_face_dataset_name)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.77726Z",
          "iopub.status.idle": "2024-04-09T05:47:02.777712Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.777488Z",
          "shell.execute_reply": "2024-04-09T05:47:02.777512Z"
        },
        "trusted": true,
        "id": "8SUUKglldsbs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "8SUUKglldsbs"
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='google/flan-t5-base'\n",
        "\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(torch_device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.779003Z",
          "iopub.status.idle": "2024-04-09T05:47:02.779394Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.77921Z",
          "shell.execute_reply": "2024-04-09T05:47:02.779225Z"
        },
        "trusted": true,
        "id": "c1CjeFe0dsbs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "c1CjeFe0dsbs"
    },
    {
      "cell_type": "code",
      "source": [
        "def number_of_trainable_model_parameters(model):\n",
        "        trainable_model_params = 0\n",
        "        all_model_params = 0\n",
        "        for _, param in model.named_parameters():\n",
        "            all_model_params += param.numel()\n",
        "            if param.requires_grad:\n",
        "                trainable_model_params += param.numel()\n",
        "        result = f\"trainable model parameters: {trainable_model_params}\\n\"\n",
        "        result += f\"all model parameters: {all_model_params}\\n\"\n",
        "        result += f\"Percentage of model params: {(trainable_model_params/all_model_params)*100}\"\n",
        "        return result"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.780644Z",
          "iopub.status.idle": "2024-04-09T05:47:02.781108Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.780913Z",
          "shell.execute_reply": "2024-04-09T05:47:02.78093Z"
        },
        "trusted": true,
        "id": "t1ZFwp6Idsbs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "t1ZFwp6Idsbs"
    },
    {
      "cell_type": "code",
      "source": [
        "print(number_of_trainable_model_parameters(original_model))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.783639Z",
          "iopub.status.idle": "2024-04-09T05:47:02.78403Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.783843Z",
          "shell.execute_reply": "2024-04-09T05:47:02.783859Z"
        },
        "trusted": true,
        "id": "WTORkqyRdsbs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "WTORkqyRdsbs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the Model with Zero Shot Inferencing"
      ],
      "metadata": {
        "id": "x8o71Cc2dsbs"
      },
      "id": "x8o71Cc2dsbs"
    },
    {
      "cell_type": "code",
      "source": [
        "index = 200\n",
        "\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    original_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "dash_line = \"-\".join(\"\" for x in range(100))\n",
        "print(dash_line)\n",
        "print(f\"Input Prompt:\\n{prompt}\")\n",
        "print(dash_line)\n",
        "print(f\"Baseline Human Summary:\\n{summary}\\n\")\n",
        "print(dash_line)\n",
        "print(f\"Model Generation - Zero Shot: \\n{output}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.785107Z",
          "iopub.status.idle": "2024-04-09T05:47:02.78552Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.785298Z",
          "shell.execute_reply": "2024-04-09T05:47:02.785315Z"
        },
        "trusted": true,
        "id": "_SYvaCv2dsbs"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_SYvaCv2dsbs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Full Fine-Tunning"
      ],
      "metadata": {
        "id": "JxsC751Kdsbs"
      },
      "id": "JxsC751Kdsbs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess the Dialog-Summary dataset"
      ],
      "metadata": {
        "id": "-dfBJzo_dsbt"
      },
      "id": "-dfBJzo_dsbt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with 'Summarize the following conversation' and the start of the summary with 'Summary as follows'"
      ],
      "metadata": {
        "id": "78mdKE_8dsbt"
      },
      "id": "78mdKE_8dsbt"
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
        "    end_prompt = '\\n\\nSummary: '\n",
        "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example['dialogue']]\n",
        "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return example"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.78707Z",
          "iopub.status.idle": "2024-04-09T05:47:02.787489Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.787282Z",
          "shell.execute_reply": "2024-04-09T05:47:02.787298Z"
        },
        "trusted": true,
        "id": "thwsN3eWdsbt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "thwsN3eWdsbt"
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset actually contains 3 diff splits: train, validation, test\n",
        "# The tokenize_function code is handling all data accross all splits in batches\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.788514Z",
          "iopub.status.idle": "2024-04-09T05:47:02.788905Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.788704Z",
          "shell.execute_reply": "2024-04-09T05:47:02.78872Z"
        },
        "trusted": true,
        "id": "uRecQwTSdsbt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "uRecQwTSdsbt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save some time, we will subsample the dataset:"
      ],
      "metadata": {
        "id": "Twil6Zw7dsbt"
      },
      "id": "Twil6Zw7dsbt"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.78976Z",
          "iopub.status.idle": "2024-04-09T05:47:02.790124Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.789942Z",
          "shell.execute_reply": "2024-04-09T05:47:02.789957Z"
        },
        "trusted": true,
        "id": "NLb5Kkqjdsbt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "NLb5Kkqjdsbt"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Shapes of the datasets:\")\n",
        "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
        "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
        "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
        "\n",
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.791251Z",
          "iopub.status.idle": "2024-04-09T05:47:02.791633Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.791433Z",
          "shell.execute_reply": "2024-04-09T05:47:02.791462Z"
        },
        "trusted": true,
        "id": "vhMdqiotdsbt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "vhMdqiotdsbt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tune the model with the Preprocessed Dataset"
      ],
      "metadata": {
        "id": "9e4mUWIfdsbt"
      },
      "id": "9e4mUWIfdsbt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now utilize the built-in Hugging Face Trainer class."
      ],
      "metadata": {
        "id": "ko8BdJlbdsbt"
      },
      "id": "ko8BdJlbdsbt"
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f\"./dialogue-summary-training-{str(int(time.time()))}\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=1e-5,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=1,\n",
        "    max_steps=1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=original_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation']\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.793321Z",
          "iopub.status.idle": "2024-04-09T05:47:02.793706Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.79352Z",
          "shell.execute_reply": "2024-04-09T05:47:02.793535Z"
        },
        "trusted": true,
        "id": "wumVxOEJdsbt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wumVxOEJdsbt"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.794812Z",
          "iopub.status.idle": "2024-04-09T05:47:02.795198Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.795012Z",
          "shell.execute_reply": "2024-04-09T05:47:02.795027Z"
        },
        "trusted": true,
        "id": "Uu8rclWCdsbt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Uu8rclWCdsbt"
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_model = AutoModelForSeq2SeqLM.from_pretrained('full/').to(torch_device)\n",
        "original_model = original_model.to(torch_device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.796187Z",
          "iopub.status.idle": "2024-04-09T05:47:02.796596Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.796367Z",
          "shell.execute_reply": "2024-04-09T05:47:02.796381Z"
        },
        "trusted": true,
        "id": "w6ptodKQdsbt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "w6ptodKQdsbt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Model Qualitatively"
      ],
      "metadata": {
        "id": "Q-9DWsh0dsbt"
      },
      "id": "Q-9DWsh0dsbt"
    },
    {
      "cell_type": "code",
      "source": [
        "index = 200\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "human_baseline_summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "\n",
        "original_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "original_text_output = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "instruct_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "instruct_text_output = tokenizer.decode(instruct_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "dash_line = \"-\".join(\"\" for x in range(100))\n",
        "print(dash_line)\n",
        "print(f\"Input Prompt:\\n{prompt}\")\n",
        "print(dash_line)\n",
        "print(f\"Baseline Human Summary:\\n{human_baseline_summary}\\n\")\n",
        "print(dash_line)\n",
        "print(f\"Original Model Generation - Zero Shot: \\n{original_text_output}\")\n",
        "print(dash_line)\n",
        "print(f\"Instruct Model Generation - Fine Tune: \\n{instruct_text_output}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.797564Z",
          "iopub.status.idle": "2024-04-09T05:47:02.797935Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.797743Z",
          "shell.execute_reply": "2024-04-09T05:47:02.797757Z"
        },
        "trusted": true,
        "id": "ppopv3Xkdsbt"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ppopv3Xkdsbt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the Model Quantitatively (with ROUGE Metric)"
      ],
      "metadata": {
        "id": "Zdz_obL-dsbu"
      },
      "id": "Zdz_obL-dsbu"
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load('rouge')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.799813Z",
          "iopub.status.idle": "2024-04-09T05:47:02.800235Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.800039Z",
          "shell.execute_reply": "2024-04-09T05:47:02.800056Z"
        },
        "trusted": true,
        "id": "M3RfsEVNdsbu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "M3RfsEVNdsbu"
    },
    {
      "cell_type": "code",
      "source": [
        "dialogue = dataset['test'][0:10]['dialogue']\n",
        "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
        "\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "for _, dialogue in enumerate(dialogue):\n",
        "    prompt = f\"\"\"\n",
        "Summarize the following conversation\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "\n",
        "    original_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    original_text_output = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
        "    original_model_summaries.append(original_text_output)\n",
        "\n",
        "    instruct_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    instruct_text_output = tokenizer.decode(instruct_outputs[0], skip_special_tokens=True)\n",
        "    instruct_model_summaries.append(instruct_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns=['human', 'original', 'instruct'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.801367Z",
          "iopub.status.idle": "2024-04-09T05:47:02.801805Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.801608Z",
          "shell.execute_reply": "2024-04-09T05:47:02.801624Z"
        },
        "trusted": true,
        "id": "2wOdmqGAdsbu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "2wOdmqGAdsbu"
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.803167Z",
          "iopub.status.idle": "2024-04-09T05:47:02.803617Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.80338Z",
          "shell.execute_reply": "2024-04-09T05:47:02.803396Z"
        },
        "trusted": true,
        "id": "diXkPeTzdsbu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "diXkPeTzdsbu"
    },
    {
      "cell_type": "code",
      "source": [
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries,\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.804667Z",
          "iopub.status.idle": "2024-04-09T05:47:02.805081Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.804876Z",
          "shell.execute_reply": "2024-04-09T05:47:02.8049Z"
        },
        "trusted": true,
        "id": "4Wp4OfvZdsbu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4Wp4OfvZdsbu"
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True\n",
        ")\n",
        "\n",
        "print(f\"Original Model: \\n{original_model_results}\")\n",
        "print(f\"Instruct Model: \\n{instruct_model_results}\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.806732Z",
          "iopub.status.idle": "2024-04-09T05:47:02.807154Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.806955Z",
          "shell.execute_reply": "2024-04-09T05:47:02.806971Z"
        },
        "trusted": true,
        "id": "b_X60yuBdsbu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "b_X60yuBdsbu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter Efficient Fine Tunning with LoRA"
      ],
      "metadata": {
        "id": "-iHiookYdsbu"
      },
      "id": "-iHiookYdsbu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets perform Parameter Efficient Fine-Tunning (PEFT). Opposed to full fine tunning, PEFT is a form of instruction fine-tunnin hat is much more efficient than full fine-tunning - with comparable evaluation results as you will see soon.\n",
        "\n",
        "PEFT is a generic term that includes Low-Rank Adaptation (LoRA) and prompt tunning (which is not the same as prompt engineering). In most cases when someone says PEFT, they typically mean LoRA, at a very high level allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU)."
      ],
      "metadata": {
        "id": "qaFQIHUFdsbu"
      },
      "id": "qaFQIHUFdsbu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the PEFT/LoRA model for Fine-Tunning"
      ],
      "metadata": {
        "id": "xcxVLeSEdsbu"
      },
      "id": "xcxVLeSEdsbu"
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.808796Z",
          "iopub.status.idle": "2024-04-09T05:47:02.809213Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.809016Z",
          "shell.execute_reply": "2024-04-09T05:47:02.809033Z"
        },
        "trusted": true,
        "id": "8qIAyLHrdsbu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "8qIAyLHrdsbu"
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    target_modules=['q','v'],\n",
        "    lora_dropout=0.05,\n",
        "    bias='none',\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.810417Z",
          "iopub.status.idle": "2024-04-09T05:47:02.81086Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.810663Z",
          "shell.execute_reply": "2024-04-09T05:47:02.81068Z"
        },
        "trusted": true,
        "id": "JXcBa9lUdsbu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "JXcBa9lUdsbu"
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(original_model, lora_config)\n",
        "print(number_of_trainable_model_parameters(peft_model))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.813138Z",
          "iopub.status.idle": "2024-04-09T05:47:02.813619Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.813365Z",
          "shell.execute_reply": "2024-04-09T05:47:02.813382Z"
        },
        "trusted": true,
        "id": "62ObyTiNdsbu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "62ObyTiNdsbu"
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f\"./peft-dialogue-summary-training-{str(int(time.time()))}\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    auto_find_batch_size=True,\n",
        "    output_dir=output_dir,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=1,\n",
        "    max_steps=1\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation']\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.815238Z",
          "iopub.status.idle": "2024-04-09T05:47:02.815699Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.81549Z",
          "shell.execute_reply": "2024-04-09T05:47:02.815508Z"
        },
        "trusted": true,
        "id": "uXSanykWdsbu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "uXSanykWdsbu"
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer.train()"
      ],
      "metadata": {
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.816695Z",
          "iopub.status.idle": "2024-04-09T05:47:02.817108Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.816914Z",
          "shell.execute_reply": "2024-04-09T05:47:02.816931Z"
        },
        "trusted": true,
        "id": "e_OE6Dk6dsbu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "e_OE6Dk6dsbu"
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(\n",
        "    peft_model_base,\n",
        "    \"peft/\"\n",
        ").to(torch_device)\n",
        "original_model = original_model.to(torch_device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.818735Z",
          "iopub.status.idle": "2024-04-09T05:47:02.81913Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.818943Z",
          "shell.execute_reply": "2024-04-09T05:47:02.818958Z"
        },
        "trusted": true,
        "id": "Ai160MRYdsbu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Ai160MRYdsbu"
    },
    {
      "cell_type": "code",
      "source": [
        "index = 200\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "human_baseline_summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "\n",
        "original_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "original_text_output = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "peft_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "peft_text_output = tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "dash_line = \"-\".join(\"\" for x in range(100))\n",
        "print(dash_line)\n",
        "print(f\"Input Prompt:\\n{prompt}\")\n",
        "print(dash_line)\n",
        "print(f\"Baseline Human Summary:\\n{human_baseline_summary}\\n\")\n",
        "print(dash_line)\n",
        "print(f\"Original Model Generation - Zero Shot: \\n{original_text_output}\")\n",
        "print(dash_line)\n",
        "print(f\"Instruct Model Generation - Zero Shot: \\n{peft_text_output}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.820671Z",
          "iopub.status.idle": "2024-04-09T05:47:02.821063Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.820865Z",
          "shell.execute_reply": "2024-04-09T05:47:02.820888Z"
        },
        "trusted": true,
        "id": "fSCsVN9Rdsbu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "fSCsVN9Rdsbu"
    },
    {
      "cell_type": "code",
      "source": [
        "dialogue = dataset['test'][0:10]['dialogue']\n",
        "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
        "\n",
        "original_model_summaries = []\n",
        "peft_model_summaries = []\n",
        "for _, dialogue in enumerate(dialogue):\n",
        "    prompt = f\"\"\"\n",
        "Summarize the following conversation\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "\n",
        "    peft_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    peft_text_output = tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
        "    peft_model_summaries.append(peft_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns=['human', 'original', 'peft'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.822294Z",
          "iopub.status.idle": "2024-04-09T05:47:02.822713Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.822524Z",
          "shell.execute_reply": "2024-04-09T05:47:02.82254Z"
        },
        "trusted": true,
        "id": "T3OGfAiNdsbu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "T3OGfAiNdsbu"
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_results = rouge.compute(\n",
        "    predictions=peft_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True\n",
        ")\n",
        "\n",
        "print(f\"Original Model: \\n{original_model_results}\")\n",
        "print(f\"Instruct Model: \\n{instruct_model_results}\")\n",
        "print(f\"Peft Model: \\n{peft_model_results}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.823706Z",
          "iopub.status.idle": "2024-04-09T05:47:02.824083Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.823899Z",
          "shell.execute_reply": "2024-04-09T05:47:02.823914Z"
        },
        "trusted": true,
        "id": "o9lHathDdsbu"
      },
      "execution_count": null,
      "outputs": [],
      "id": "o9lHathDdsbu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Knowledge Grounding**\n",
        "\n",
        "Knowledge grounding in Natural Language Processing (NLP) refers to the process of connecting or linking information in text to real-world entities or concepts. It involves ensuring that the language model understands and can relate the information it processes to factual, contextual, or external knowledge.\n",
        "\n",
        "For instance, if a sentence mentions \"Einstein's theory of relativity,\" knowledge grounding would involve the model recognizing that Einstein refers to a renowned physicist and the theory of relativity is a fundamental concept in physics.\n",
        "\n",
        "Knowledge grounding is crucial for NLP applications that require a deeper understanding of the world, as it enables the model to go beyond surface-level patterns in text and make meaningful inferences based on its understanding of the underlying concepts. This is particularly important in tasks like question-answering, where the model needs to provide accurate and contextually relevant responses.\n",
        "Knowledge grounding with Retrieval Augmented Generation (RAG) is implemented to mitigate hallucinations and provide trustworthy and reliable responses. This is achieved by incorporating information from external sources to validate and support the generated text.\n"
      ],
      "metadata": {
        "id": "yd4gjAMHdsbv"
      },
      "id": "yd4gjAMHdsbv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## To know more"
      ],
      "metadata": {
        "id": "G-Z_877Edsbv"
      },
      "id": "G-Z_877Edsbv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG"
      ],
      "metadata": {
        "id": "zztciBWidsbv"
      },
      "id": "zztciBWidsbv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/build-industry-specific-llms-using-retrieval-augmented-generation-af9e98bb6f68"
      ],
      "metadata": {
        "id": "Bt8cRiHHdsbv"
      },
      "id": "Bt8cRiHHdsbv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langchain"
      ],
      "metadata": {
        "id": "fzvEInnUdsbv"
      },
      "id": "fzvEInnUdsbv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://python.langchain.com/docs/get_started/introduction"
      ],
      "metadata": {
        "id": "DhycADpadsbv"
      },
      "id": "DhycADpadsbv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI API Key\n",
        "\n",
        "Note: Please note that the use of the OpenAI API may require the utilization of allocated free credits or additional purchases for implementing the project. Kindly take note of the free credits limit provided for your usage."
      ],
      "metadata": {
        "id": "vUr7VzLXdsbv"
      },
      "id": "vUr7VzLXdsbv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.howtogeek.com/885918/how-to-get-an-openai-api-key/#:~:text=Go%20to%20OpenAI's%20Platform%20website,generate%20a%20new%20API%20key."
      ],
      "metadata": {
        "id": "TWpHI88ddsbv"
      },
      "id": "TWpHI88ddsbv"
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run llm_app.py"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-09T05:47:02.825295Z",
          "iopub.status.idle": "2024-04-09T05:47:02.825679Z",
          "shell.execute_reply.started": "2024-04-09T05:47:02.825493Z",
          "shell.execute_reply": "2024-04-09T05:47:02.825509Z"
        },
        "trusted": true,
        "id": "tvliMvcmdsbv"
      },
      "execution_count": null,
      "outputs": [],
      "id": "tvliMvcmdsbv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**\n",
        "\n",
        "In today's world, we see language models doing some pretty amazing things. They help businesses understand text on a big scale and make our online experiences better.\n",
        "\n",
        "This project was all about getting to know these language models inside out. We looked at how they work, how we can use them better, and even gave them a bit of a tune-up. We made them smarter by teaching them how to answer questions and summarize conversations.\n",
        "\n",
        "We also built a cool shopping chatbot. This bot is smart because it doesn't make up stuff; it gives you real info about products.\n",
        "\n",
        "With this project, you've learned a lot about these language models, and you're all set to use them for exciting tasks, from writing text to building smart applications.\n",
        "\n",
        "\n",
        "Start experimenting !!"
      ],
      "metadata": {
        "id": "tKMsogQXdsbv"
      },
      "id": "tKMsogQXdsbv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Interview Questions**\n",
        "\n",
        "\n",
        "* Can you explain the significance of Large Language Models in today's data-driven world?\n",
        "* What is the main purpose of prompt engineering in the context of LLMs?\n",
        "* How does fine-tuning enhance the performance of a pre-trained language model?\n",
        "* What is the difference between full fine-tuning and Parameter Efficient Fine Tuning (PEFT)?\n",
        "* Can you explain the concept of Retrieval Augmented Generation (RAG) and how it was implemented in the project?\n",
        "* How does knowledge grounding improve the reliability of responses generated by the chatbot?\n",
        "* What is Reinforcement Learning from Human Feedback (RLHF)?\n",
        "\n",
        "* How did you evaluate the performance of the models in this project, and what metrics did you use?\n",
        "How did you handle the potential issue of hallucinations in the chatbot's responses?\n",
        "* What challenges did you encounter during the implementation of fine-tuning techniques, and how did you overcome them?\n",
        "* How would you address potential ethical concerns related to the use of language models in real-world applications?\n",
        "* What are some potential future improvements or extensions you would consider for this project?"
      ],
      "metadata": {
        "id": "xrog8qqIdsbv"
      },
      "id": "xrog8qqIdsbv"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sNz6l3Pedsbv"
      },
      "id": "sNz6l3Pedsbv"
    }
  ]
}